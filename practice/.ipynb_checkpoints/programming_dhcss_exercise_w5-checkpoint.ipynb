{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4jpcIgJ_gwv"
   },
   "source": [
    "Programming for DHCSS Exercises: Week 5. Dictionaries, Sets, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlJeHHI4kcnV"
   },
   "outputs": [],
   "source": [
    "### Problem 1 ###\n",
    "\n",
    "### First download data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/taegyoon-kim/programming_dhcss_23fw/main/week_5/digital_media_comments.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "\n",
    "### Problem 1a: Which platform has most unique users?\n",
    "\n",
    "### Hint: From the columns in the Pandas dataframe, create sets for respective platforms\n",
    "\n",
    "### Hint: Here is a quick summary of how to subset rows conditional on a column from a Pandas dataframe\n",
    "\n",
    "df_fb = df[df['Platform'] == 'Facebook'] # return rows whose value for the \"Platform\" column is \"Facebook\"\n",
    "df_fb['Identifier'] # access \"Identifier\" column\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "### Problem 1b: Which platform has most unique users whose posts are flagged as hateful?\n",
    "\n",
    "### Hint: When you need multiple conditions to subset data from a Pandas dataframe, connect them using '&'\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "### Problem 1c: Get the numbers of 1) the users who have commented on both Facebook and Twitter 2) the users who have commneted on Instagram but not on YouTube\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "### Problem 1d: Generate a nested dictionary with each inner dictionary representing a unique user, where the values inside the inner dictionaries represent the number of comments from each platform\n",
    "\n",
    "### Start with a subset of the data for hateful posts\n",
    "\n",
    "df_hate = df[df['Flagged_Hateful'] == 'Yes']\n",
    "\n",
    "### Hint: Use df.iterrows() (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html)\n",
    "\n",
    "### Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "XV97r5rFlIKe"
   },
   "outputs": [],
   "source": [
    "### Problem 2 ###\n",
    "\n",
    "### Problem 2a: Join the following dictionaries (keep the orgiianl dictionaries intact though!)\n",
    "\n",
    "dict1 = {'Japan': 'Tokyo', 'U.S.A.': 'Washington D.C.', 'Indonesia': 'Jarkarta'}\n",
    "dict2 = {'Canada': 'Ottawa', 'South Korea': 'Seoul', 'China': 'Beijing'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Problem 2b: Create a new dictionary where the keys are capitals and the values are their corresponding countries\n",
    "\n",
    "### Hint: Use dictionary comprehension\n",
    "\n",
    "### Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 2a: Join the following dictionaries (keep the orgiianl dictionaries intact though!)\n",
    "dict3 = dict(dict1, **dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 2b: Create a new dictionary where the keys are capitals and the values are their corresponding countries\n",
    "\n",
    "### Hint: Use dictionary comprehension\n",
    "\n",
    "### Write your code here\n",
    "new_dict = {}\n",
    "for key, item in dict3.items():\n",
    "    new_dict[item] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokyo': 'Japan',\n",
       " 'Washington D.C.': 'U.S.A.',\n",
       " 'Jarkarta': 'Indonesia',\n",
       " 'Ottawa': 'Canada',\n",
       " 'Seoul': 'South Korea',\n",
       " 'Beijing': 'China'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{item:key for key, item in dict3.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtYVVzECfoDU",
    "outputId": "44ba16d1-f003-4ade-d278-760d6c5e9765"
   },
   "outputs": [],
   "source": [
    "### Problem 3 ###\n",
    "\n",
    "### The following lists contain some data about architets and their works\n",
    "\n",
    "architects = [\n",
    "    'Frank Lloyd Wright',\n",
    "    'Le Corbusier',\n",
    "    'Zaha Hadid',\n",
    "    'Norman Foster',\n",
    "    'Renzo Piano',\n",
    "    'Buckminster Fuller',\n",
    "    'Philip Johnson',\n",
    "    'I. M. Pei',\n",
    "    'Frank Gehry',\n",
    "    'Alvar Aalto',\n",
    "    'Louis Sullivan',\n",
    "    'Rem Koolhaas',\n",
    "    'Antoni Gaudí',\n",
    "    'Ludwig Hilberseimer'\n",
    "]\n",
    "\n",
    "works = [\n",
    "    'Fallingwater',\n",
    "    'Villa Savoye',\n",
    "    'London Aquatics Centre',\n",
    "    'The Gherkin',\n",
    "    'The Shard',\n",
    "    'Geodesic Dome',\n",
    "    'The Glass House',\n",
    "    'Louvre Pyramid',\n",
    "    'Guggenheim Museum Bilbao',\n",
    "    'Villa Mairea',\n",
    "    'Wainwright Building',\n",
    "    'CCTV Headquarters',\n",
    "    'Sagrada Família',\n",
    "    'LaFayette Park'\n",
    "]\n",
    "\n",
    "birth_years = [\n",
    "    1867, 1887, 1950, 1935, 1937, 1895, 1906, 1917, 1929, 1898, 1856, 1944, 1852, 1885\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'W': 1, 'C': 1, 'H': 2, 'F': 2, 'P': 2, 'J': 1, 'G': 2, 'A': 1, 'S': 1, 'K': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 3a: Convert the first two lists into a dictionary (architect-work pairs)\n",
    "\n",
    "### Write your code here\n",
    "arwo = dict(zip(architects,works))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 3b: Repeat but 1a but only include architect born after 1901\n",
    "\n",
    "### Write your code here\n",
    "len(arwo)\n",
    "\n",
    "yrarwo = dict(zip(birth_years, [arwo]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1867: {'Alvar Aalto': 'Villa Mairea',\n",
      "        'Antoni Gaudí': 'Sagrada Família',\n",
      "        'Buckminster Fuller': 'Geodesic Dome',\n",
      "        'Frank Gehry': 'Guggenheim Museum Bilbao',\n",
      "        'Frank Lloyd Wright': 'Fallingwater',\n",
      "        'I. M. Pei': 'Louvre Pyramid',\n",
      "        'Le Corbusier': 'Villa Savoye',\n",
      "        'Louis Sullivan': 'Wainwright Building',\n",
      "        'Ludwig Hilberseimer': 'LaFayette Park',\n",
      "        'Norman Foster': 'The Gherkin',\n",
      "        'Philip Johnson': 'The Glass House',\n",
      "        'Rem Koolhaas': 'CCTV Headquarters',\n",
      "        'Renzo Piano': 'The Shard',\n",
      "        'Zaha Hadid': 'London Aquatics Centre'}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(yrarwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "### Problem 3c: Using a dictionary, count the number of architects whose last names start with the same letter\n",
    "\n",
    "### Hint: Each key would be the last name, and the corresponding value would be the number of times the last name appear in the data\n",
    "\n",
    "### Write your code here\n",
    "cnt = 0\n",
    "for key, value in arwo.items():\n",
    "#     print(key.split()[0])\n",
    "    if key.split()[0][0].lower()==key.split()[0][-1]:\n",
    "        cnt += 1\n",
    "                            \n",
    "    \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Dtbz7lWVckqL",
    "outputId": "c40dc5f6-8a6c-4494-a6d3-b8f391367923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 798 JSON objects successfully.\n"
     ]
    }
   ],
   "source": [
    "### Problem 4 ###\n",
    "\n",
    "### Let's read a JSON file for tweets written by Tulsi Gabbard, a former U.S. House member\n",
    "### The following code will return a list of dictionaries with each dictionary representing a tweet\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/taegyoon-kim/programming_dhcss_23fw/main/week_5/RepAdamSmith.json\"\n",
    "response = requests.get(url) # send a GET request to fetch the JSON data\n",
    "\n",
    "lines = response.text.splitlines() # help(str.splitlines)\n",
    "\n",
    "l_tweets = []\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        l_tweets.append(tweet)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Loaded {len(l_tweets)} JSON objects successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n"
     ]
    }
   ],
   "source": [
    "### Problem 4a: \n",
    "# From the list of dictionaries, create a Pandas dataframe containing \"text\", \"id_str\", \" and \"favorite_count\" \n",
    "# as columns\n",
    "\n",
    "### Hint I: You can use try-except to handle situations where the key you are looking for does not exist in the dictionary\n",
    "\n",
    "### Hint II: Loop through dictionaries (tweets) in \"l_tweets\"\n",
    "\n",
    "### Hint III: At each iteration, create a small dictionary containg the required keys and append it to an empty list\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "# result=pd.DataFrame({  'text':l_tweets[\"text\"]\n",
    "#                 , 'id_str':l_tweets[\"id_str\"]\n",
    "#                 , 'favorite_count' : l_tweets[\"favorite_count\"]\n",
    "#                  })\n",
    "\n",
    "empty_lst = []\n",
    "for l_tweet in l_tweets:\n",
    "    try:\n",
    "#         print(l_tweet)\n",
    "        empty_lst.append({  'text':l_tweet[\"text\"]\n",
    "                    , 'id_str':l_tweet[\"id_str\"]\n",
    "                    , 'favorite_count' : l_tweet[\"favorite_count\"]\n",
    "                     })\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text              id_str  \\\n",
      "0    ICYMI: I was on @cspan's Newsmakers to discuss...  410112352747388928   \n",
      "1    Congrats to @WoodbyWY for being named one of 5...  410168353953570816   \n",
      "2    Honored to attend Rainier Avenue ribbon-cuttin...  410476710950227968   \n",
      "3    Honored to receive 100% voting record on food ...  410523041773281280   \n",
      "4    #Budget deal is far from perfect but provides ...  410787420334878721   \n",
      "..                                                 ...                 ...   \n",
      "440  We can keep gov't open w/out gratuitous measur...  543160458203918337   \n",
      "441  RT @seattledot: Inclement weather on its way. ...  543207434714906624   \n",
      "442  RT @WaysMeansCmte: .@repsandylevin, @RepAdamSm...  544920453262082048   \n",
      "443  RT @ACLU_WA: BREAKING:  Justice Department ann...  545696754768027648   \n",
      "444  I applaud POTUS for creating Task Force on 21s...  546053281219624960   \n",
      "\n",
      "     favorite_count  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 2  \n",
      "4                 0  \n",
      "..              ...  \n",
      "440               5  \n",
      "441               0  \n",
      "442               0  \n",
      "443               0  \n",
      "444               6  \n",
      "\n",
      "[445 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame.from_dict(empty_lst)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 4b: See what the get() dictionary method does: https://www.geeksforgeeks.org/python-dictionary-get-method/\n",
    "\n",
    "### Use this method instead of try-except\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "\n",
    "empty_lst = []\n",
    "for l_tweet in l_tweets:\n",
    "    empty_lst.append({  'text':l_tweet.get(\"text\")\n",
    "                    , 'id_str':l_tweet.get(\"id_str\")\n",
    "                    , 'favorite_count' : l_tweet.get(\"favorite_count\")\n",
    "                     })\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
