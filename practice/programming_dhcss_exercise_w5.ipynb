{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4jpcIgJ_gwv"
   },
   "source": [
    "Programming for DHCSS Exercises: Week 5. Dictionaries, Sets, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "wlJeHHI4kcnV"
   },
   "outputs": [],
   "source": [
    "### Problem 1 ###\n",
    "\n",
    "### First download data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/taegyoon-kim/programming_dhcss_23fw/main/week_5/digital_media_comments.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "### Problem 1c: Get the numbers of 1) the users who have commented on both Facebook and Twitter 2) the users who have commneted on Instagram but not on YouTube\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "### Problem 1d: Generate a nested dictionary with each inner dictionary representing a unique user, where the values inside the inner dictionaries represent the number of comments from each platform\n",
    "\n",
    "### Start with a subset of the data for hateful posts\n",
    "\n",
    "df_hate = df[df['Flagged_Hateful'] == 'Yes']\n",
    "\n",
    "### Hint: Use df.iterrows() (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html)\n",
    "\n",
    "### Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 1a: Which platform has most unique users?\n",
    "\n",
    "### Hint: From the columns in the Pandas dataframe, create sets for respective platforms\n",
    "\n",
    "### Hint: Here is a quick summary of how to subset rows conditional on a column from a Pandas dataframe\n",
    "\n",
    "df_fb = df[df['Platform'] == 'Facebook'] # return rows whose value for the \"Platform\" column is \"Facebook\"\n",
    "df_fb['Identifier'] # access \"Identifier\" column\n",
    "import numpy as np\n",
    "### Write your code here\n",
    "df_why = df_fb.copy()\n",
    "df_why['Identifier_cnt'] = df_why['Identifier'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_why['Identifier_cnt'] = df_why['Identifier_cnt'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Flagged_Hateful</th>\n",
       "      <th>Identifier_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>550</td>\n",
       "      <td>Comment 52</td>\n",
       "      <td>2021-05-13</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>No</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>142</td>\n",
       "      <td>Comment 678</td>\n",
       "      <td>2022-02-09</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>No</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Identifier      Comment        Date  Platform Flagged_Hateful  \\\n",
       "52          550   Comment 52  2021-05-13  Facebook              No   \n",
       "678         142  Comment 678  2022-02-09  Facebook              No   \n",
       "\n",
       "     Identifier_cnt  \n",
       "52              3.0  \n",
       "678             3.0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_why[df_why['Identifier_cnt']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 1b: Which platform has most unique users whose posts are flagged as hateful?\n",
    "\n",
    "### Hint: When you need multiple conditions to subset data from a Pandas dataframe, connect them using '&'\n",
    "\n",
    "### Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "XV97r5rFlIKe"
   },
   "outputs": [],
   "source": [
    "### Problem 2 ###\n",
    "\n",
    "### Problem 2a: Join the following dictionaries (keep the orgiianl dictionaries intact though!)\n",
    "\n",
    "dict1 = {'Japan': 'Tokyo', 'U.S.A.': 'Washington D.C.', 'Indonesia': 'Jarkarta'}\n",
    "dict2 = {'Canada': 'Ottawa', 'South Korea': 'Seoul', 'China': 'Beijing'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Problem 2b: Create a new dictionary where the keys are capitals and the values are their corresponding countries\n",
    "\n",
    "### Hint: Use dictionary comprehension\n",
    "\n",
    "### Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 2a: Join the following dictionaries (keep the orgiianl dictionaries intact though!)\n",
    "dict3 = dict(dict1, **dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 2b: Create a new dictionary where the keys are capitals and the values are their corresponding countries\n",
    "\n",
    "### Hint: Use dictionary comprehension\n",
    "\n",
    "### Write your code here\n",
    "new_dict = {}\n",
    "for key, item in dict3.items():\n",
    "    new_dict[item] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokyo': 'Japan',\n",
       " 'Washington D.C.': 'U.S.A.',\n",
       " 'Jarkarta': 'Indonesia',\n",
       " 'Ottawa': 'Canada',\n",
       " 'Seoul': 'South Korea',\n",
       " 'Beijing': 'China'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{item:key for key, item in dict3.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtYVVzECfoDU",
    "outputId": "44ba16d1-f003-4ade-d278-760d6c5e9765"
   },
   "outputs": [],
   "source": [
    "### Problem 3 ###\n",
    "\n",
    "### The following lists contain some data about architets and their works\n",
    "\n",
    "architects = [\n",
    "    'Frank Lloyd Wright',\n",
    "    'Le Corbusier',\n",
    "    'Zaha Hadid',\n",
    "    'Norman Foster',\n",
    "    'Renzo Piano',\n",
    "    'Buckminster Fuller',\n",
    "    'Philip Johnson',\n",
    "    'I. M. Pei',\n",
    "    'Frank Gehry',\n",
    "    'Alvar Aalto',\n",
    "    'Louis Sullivan',\n",
    "    'Rem Koolhaas',\n",
    "    'Antoni Gaudí',\n",
    "    'Ludwig Hilberseimer'\n",
    "]\n",
    "\n",
    "works = [\n",
    "    'Fallingwater',\n",
    "    'Villa Savoye',\n",
    "    'London Aquatics Centre',\n",
    "    'The Gherkin',\n",
    "    'The Shard',\n",
    "    'Geodesic Dome',\n",
    "    'The Glass House',\n",
    "    'Louvre Pyramid',\n",
    "    'Guggenheim Museum Bilbao',\n",
    "    'Villa Mairea',\n",
    "    'Wainwright Building',\n",
    "    'CCTV Headquarters',\n",
    "    'Sagrada Família',\n",
    "    'LaFayette Park'\n",
    "]\n",
    "\n",
    "birth_years = [\n",
    "    1867, 1887, 1950, 1935, 1937, 1895, 1906, 1917, 1929, 1898, 1856, 1944, 1852, 1885\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'W': 1, 'C': 1, 'H': 2, 'F': 2, 'P': 2, 'J': 1, 'G': 2, 'A': 1, 'S': 1, 'K': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 3a: Convert the first two lists into a dictionary (architect-work pairs)\n",
    "\n",
    "### Write your code here\n",
    "arwo = dict(zip(architects,works))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 3b: Repeat but 1a but only include architect born after 1901\n",
    "\n",
    "### Write your code here\n",
    "len(arwo)\n",
    "\n",
    "yrarwo = dict(zip(birth_years, [arwo]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1867: {'Alvar Aalto': 'Villa Mairea',\n",
      "        'Antoni Gaudí': 'Sagrada Família',\n",
      "        'Buckminster Fuller': 'Geodesic Dome',\n",
      "        'Frank Gehry': 'Guggenheim Museum Bilbao',\n",
      "        'Frank Lloyd Wright': 'Fallingwater',\n",
      "        'I. M. Pei': 'Louvre Pyramid',\n",
      "        'Le Corbusier': 'Villa Savoye',\n",
      "        'Louis Sullivan': 'Wainwright Building',\n",
      "        'Ludwig Hilberseimer': 'LaFayette Park',\n",
      "        'Norman Foster': 'The Gherkin',\n",
      "        'Philip Johnson': 'The Glass House',\n",
      "        'Rem Koolhaas': 'CCTV Headquarters',\n",
      "        'Renzo Piano': 'The Shard',\n",
      "        'Zaha Hadid': 'London Aquatics Centre'}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(yrarwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "### Problem 3c: Using a dictionary, count the number of architects whose last names start with the same letter\n",
    "\n",
    "### Hint: Each key would be the last name, and the corresponding value would be the number of times the last name appear in the data\n",
    "\n",
    "### Write your code here\n",
    "cnt = 0\n",
    "for key, value in arwo.items():\n",
    "#     print(key.split()[0])\n",
    "    if key.split()[0][0].lower()==key.split()[0][-1]:\n",
    "        cnt += 1\n",
    "                            \n",
    "    \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Dtbz7lWVckqL",
    "outputId": "c40dc5f6-8a6c-4494-a6d3-b8f391367923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 798 JSON objects successfully.\n"
     ]
    }
   ],
   "source": [
    "### Problem 4 ###\n",
    "\n",
    "### Let's read a JSON file for tweets written by Tulsi Gabbard, a former U.S. House member\n",
    "### The following code will return a list of dictionaries with each dictionary representing a tweet\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/taegyoon-kim/programming_dhcss_23fw/main/week_5/RepAdamSmith.json\"\n",
    "response = requests.get(url) # send a GET request to fetch the JSON data\n",
    "\n",
    "lines = response.text.splitlines() # help(str.splitlines)\n",
    "\n",
    "l_tweets = []\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        l_tweets.append(tweet)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Loaded {len(l_tweets)} JSON objects successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n",
      "'favorite_count'\n"
     ]
    }
   ],
   "source": [
    "### Problem 4a: \n",
    "# From the list of dictionaries, create a Pandas dataframe containing \"text\", \"id_str\", \" and \"favorite_count\" \n",
    "# as columns\n",
    "\n",
    "### Hint I: You can use try-except to handle situations where the key you are looking for does not exist in the dictionary\n",
    "\n",
    "### Hint II: Loop through dictionaries (tweets) in \"l_tweets\"\n",
    "\n",
    "### Hint III: At each iteration, create a small dictionary containg the required keys and append it to an empty list\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "# result=pd.DataFrame({  'text':l_tweets[\"text\"]\n",
    "#                 , 'id_str':l_tweets[\"id_str\"]\n",
    "#                 , 'favorite_count' : l_tweets[\"favorite_count\"]\n",
    "#                  })\n",
    "\n",
    "empty_lst = []\n",
    "for l_tweet in l_tweets:\n",
    "    try:\n",
    "#         print(l_tweet)\n",
    "        empty_lst.append({  'text':l_tweet[\"text\"]\n",
    "                    , 'id_str':l_tweet[\"id_str\"]\n",
    "                    , 'favorite_count' : l_tweet[\"favorite_count\"]\n",
    "                     })\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors': None,\n",
       " 'truncated': False,\n",
       " 'text': 'I applaud POTUS for creating Task Force on 21st Century Policing. Tragedies in Ferguson &amp; NY show we must do better. http://t.co/7TBCla0WOX',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'id': 5.46053281219625e+17,\n",
       " 'favorite_count': 6,\n",
       " 'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
       " 'retweeted': False,\n",
       " 'coordinates': None,\n",
       " 'entities': {'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'hashtags': [],\n",
       "  'urls': [{'url': 'http://t.co/7TBCla0WOX',\n",
       "    'indices': [121, 143],\n",
       "    'expanded_url': 'http://adamsmith.house.gov/media-center/press-releases/smith-applauds-president-for-creating-task-force-on-21st-century',\n",
       "    'display_url': 'adamsmith.house.gov/media-center/p…'}]},\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'id_str': '546053281219624960',\n",
       " 'retweet_count': 3,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'favorited': False,\n",
       " 'user': {'id': 58928690, 'id_str': '58928690'},\n",
       " 'geo': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'possibly_sensitive': False,\n",
       " 'lang': 'en',\n",
       " 'created_at': 'Fri Dec 19 21:23:42 +0000 2014',\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'place': None}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text              id_str  \\\n",
      "0    ICYMI: I was on @cspan's Newsmakers to discuss...  410112352747388928   \n",
      "1    Congrats to @WoodbyWY for being named one of 5...  410168353953570816   \n",
      "2    Honored to attend Rainier Avenue ribbon-cuttin...  410476710950227968   \n",
      "3    Honored to receive 100% voting record on food ...  410523041773281280   \n",
      "4    #Budget deal is far from perfect but provides ...  410787420334878721   \n",
      "..                                                 ...                 ...   \n",
      "440  We can keep gov't open w/out gratuitous measur...  543160458203918337   \n",
      "441  RT @seattledot: Inclement weather on its way. ...  543207434714906624   \n",
      "442  RT @WaysMeansCmte: .@repsandylevin, @RepAdamSm...  544920453262082048   \n",
      "443  RT @ACLU_WA: BREAKING:  Justice Department ann...  545696754768027648   \n",
      "444  I applaud POTUS for creating Task Force on 21s...  546053281219624960   \n",
      "\n",
      "     favorite_count  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 2  \n",
      "4                 0  \n",
      "..              ...  \n",
      "440               5  \n",
      "441               0  \n",
      "442               0  \n",
      "443               0  \n",
      "444               6  \n",
      "\n",
      "[445 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame.from_dict(empty_lst)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem 4b: See what the get() dictionary method does: https://www.geeksforgeeks.org/python-dictionary-get-method/\n",
    "\n",
    "### Use this method instead of try-except\n",
    "\n",
    "### Write your code here\n",
    "\n",
    "\n",
    "\n",
    "empty_lst = []\n",
    "for l_tweet in l_tweets:\n",
    "    empty_lst.append({  'text':l_tweet.get(\"text\")\n",
    "                    , 'id_str':l_tweet.get(\"id_str\")\n",
    "                    , 'favorite_count' : l_tweet.get(\"favorite_count\")\n",
    "                     })\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
